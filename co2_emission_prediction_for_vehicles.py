# -*- coding: utf-8 -*-
"""CO2 Emission Prediction for Vehicles

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a6-Nx_ieUz0658_Ar4LQkBUmyey6sfu4
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score

from google.colab import drive
drive.mount('/content/drive')

dataset_path = '/content/drive/MyDrive/University susmita/CSE422/Project_folder_group3/Car_fuel consumption dataset.csv'
df= pd.read_csv(dataset_path)

df



print(f"Dataset shape: {df.shape}")

print("Dataset description: ")
df.describe()

print(f"dataset data type: ")

df.dtypes

df.select_dtypes(include=['object'])

"""# **Visualise**"""

import seaborn as sns
df_numeric = df.select_dtypes(include=['number'])


corr = df_numeric.corr()


plt.figure(figsize=(40, 32))
sns.heatmap(corr, annot=True, cmap='YlGnBu', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap (Cleaned Data)')
plt.show()

print("Unique Labels in the dependent column: ", df["tailpipe_co2_ft1"].unique())

print("The number of occurences of the unique labels:\n", df["tailpipe_co2_ft1"].value_counts())

class_counts = df['tailpipe_co2_ft1'].value_counts()

print(class_counts)

# Bin the classes into intervals
bins = 20  # Adjust as necessary
df['binned_classes'] = pd.cut(df['tailpipe_co2_ft1'], bins=bins)
binned_counts = df['binned_classes'].value_counts().sort_index()

# Plot the binned distribution
plt.figure(figsize=(16, 8))
plt.bar(binned_counts.index.astype(str), binned_counts.values, color="lightgreen", alpha=0.8)
plt.title("Binned Class Distribution of tailpipe_co2_ft1", fontsize=16)
plt.xlabel("Bins", fontsize=14)
plt.ylabel("Count", fontsize=14)
plt.xticks(rotation=45)
plt.show()

"""# **Dataset** **Exploration**

**Columns** **with** **Null** **Value**
"""

print("Datset Null checking: ")
df.isnull().sum()

"""## **Checking the dataset for nan, redundant and columns with all 0**

# **columns with all 0**
"""

columns_all_zeros = df.columns[(df == 0).all()]

#
zero_counts = (df== 0).sum()
columns_high_zeros = zero_counts[zero_counts > len(df) * 0.5].index


print('Columns with all zeros: ',columns_all_zeros.tolist())
print('Columns with high number of zeros: ',columns_high_zeros.tolist())
# columns_all_zeros.tolist(), columns_high_zeros.tolist()

"""## **Columns with Nan and Redundant values**"""

columns_with_nan = df.columns[df.isna().any()]
nan_counts = df[columns_with_nan].isna().sum()

#  (redundant)
redundant_columns = df.columns[df.nunique(dropna=True) == 1]


print('Columns With Nan',columns_with_nan.tolist())
print('Nan Counts',nan_counts.tolist())
print('Redundant Columns',redundant_columns.tolist())

"""# **Cleaning and Preprocessing the Dataset**"""

#before cleaning
df.shape

"""# **Dataset cleaning**"""



# Drop columns with more than 50% zeros or NaN values
threshold = len(df) * .5
df_cleaned = df.loc[:, (df.isna().sum() <= threshold) & ((df == 0).sum() <= threshold)]

# Drop columns with a single unique value (redundant columns)
df_cleaned = df_cleaned.loc[:, df_cleaned.nunique(dropna=True) > 1]
df_cleaned = df_cleaned.dropna(axis=0, how='any')
df_cleaned = df_cleaned.loc[~(df_cleaned == 0).all(axis=1)]
final_shape = df_cleaned.shape
print("Final dataset shape:", final_shape)

df_corr = df_cleaned.select_dtypes(include=['number']).corr()
df_corr

"""# Dropping Redundant column"""

numeric_df_cleaned = df_cleaned.select_dtypes(include=['number'])


correlation_matrix = numeric_df_cleaned.corr()

# correlation threshold
threshold = 0.95

# highly correlated features
cols_to_remove = set()
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > threshold:
            col_to_remove = correlation_matrix.columns[i]
            cols_to_remove.add(col_to_remove)

# Drop redundant columns from the original df_cleaned
df_cleaned = df_cleaned.drop(columns=cols_to_remove)
print(f"Dropped redundant columns based on correlation > {threshold}: {list(cols_to_remove)}")
print(f"Final dataset shape after correlation-based feature removal: {df_cleaned.shape}")

df_corr = df_cleaned.select_dtypes(include=['number']).corr()
df_corr

print("No NaN values left in the dataset:", df_cleaned.isna().sum().sum() == 0)
print("No Columns with all zero values left:", (df_cleaned == 0).all().sum() == 0)
print("No Columns with a single unique value left:", df_cleaned.nunique(dropna=True).eq(1).sum() == 0)

print("Original shape:", df.shape)
print("Cleaned shape:", df_cleaned.shape)

print(f"Original number of rows: {df.shape[0]}")
print(f"Number of rows after cleaning: {df_cleaned.shape[0]}")

print(f"Original number of columns: {df.shape[1]}")#column
print(f"Number of columns after cleaning: {df_cleaned.shape[1]}")

"""# **Outliar**"""

from scipy.stats import zscore

numerical_features = df_cleaned.select_dtypes(include=['number']).columns

z_scores = zscore(df_cleaned[numerical_features])

# Set a threshold (e.g., |Z| > 3)
outliers = (abs(z_scores) > 3).sum(axis=0)
print(f'Number of outliers per feature:\n{outliers}')

# Remove rows with outliers
df_cleaned_capped = df_cleaned[(abs(z_scores) <= 3).all(axis=1)]
print(f"Dataset shape after outlier removal with z: {df_cleaned_capped.shape}")

#IR

# Calculate the Q1, Q3, and IQR for each numerical feature
Q1 = df_cleaned[numerical_features].quantile(0.25)
Q3 = df_cleaned[numerical_features].quantile(0.75)
IQR = Q3 - Q1


df_cleaned_capped1 = df_cleaned.copy()

# Apply cap for the outliers (above Q3 + 1.5*IQR and below Q1 - 1.5*IQR)
for feature in numerical_features:
    df_cleaned_capped1[feature] = df_cleaned[feature].clip(lower=Q1[feature] - 1.5 * IQR[feature],
                                                          upper=Q3[feature] + 1.5 * IQR[feature])


print(f'Outliers capped in the dataset.')
print(f'Dataset shape after outlier capping with IR: {df_cleaned_capped1.shape}')
if df_cleaned_capped1.shape[0]<=df_cleaned_capped.shape[0]:
  df_cleaned = df_cleaned_capped1
else:
  df_cleaned = df_cleaned_capped

print(f'new_df_cleaned: {df_cleaned.shape}')

df_cleaned.dtypes

"""## **Correlation**"""

df_corr = df_cleaned.select_dtypes(include=['number']).corr()
df_corr



df_cleaned_numeric = df_cleaned.select_dtypes(include=['number'])

corr_cleaned = df_cleaned_numeric.corr()

plt.figure(figsize=(12, 8))
sns.heatmap(corr_cleaned, annot=True, cmap='YlGnBu', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap (Cleaned Data)')
plt.show()

df_cleaned_numeric = df_cleaned.select_dtypes(include=['number'])
df_cleaned_numeric.corr()

"""# **Target Feature**

# **Identifying the most corelated feature Data & Scaling**
"""

import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split

import numpy as np
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

print('Shape before dropping: ',df_cleaned.shape)
df_cleaned['tailpipe_co2_ft1'] = df_cleaned['tailpipe_co2_ft1'].replace(-1, np.nan)


imputer = SimpleImputer(strategy='mean')
df_cleaned['tailpipe_co2_ft1'] = imputer.fit_transform(df_cleaned[['tailpipe_co2_ft1']])

X = df_cleaned.drop(['tailpipe_co2_ft1', 'engine_index', 'annual_fuel_cost_ft1', 'annual_consumption_in_barrels_ft1'], axis=1)
y = df_cleaned['tailpipe_co2_ft1']


categorical_features = X.select_dtypes(include=['object']).columns
numerical_features = X.select_dtypes(include=['int64', 'float64']).columns


ct = ColumnTransformer([
    ('onehot', OneHotEncoder(handle_unknown='ignore'), categorical_features),
    ('scaler', StandardScaler(), numerical_features)
])
print(X.shape)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

"""## **Plot model method**"""

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import seaborn as sns
import matplotlib.pyplot as plt

def plot_model_performance(y_test, y_pred, model_name):


    # Actual vs Predicted scatter plot
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=y_test, y=y_pred, alpha=0.7, color='purple', s=100, edgecolor='black', linewidth=0.8)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', linewidth=2)
    plt.title(f"Actual vs Predicted for {model_name}", fontsize=16, fontweight='bold')
    plt.xlabel("Actual", fontsize=12)
    plt.ylabel("Predicted", fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.show()

def print_model_metrics(y_test, y_pred, model_name):


    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    import numpy as np

    # Calculate metrics
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"Predictions: {y_pred[:5]}")
    print(f"Actual values: {y_test[:5].values}")
    print(f"Accuracy (R²): {r2:.4f}")
    print(f"Mean Absolute Error: {mae:.4f}")
    print(f"Mean Squared Error: {mse:.4f}")
    print(f"Root Mean Squared Error: {rmse:.4f}")
    print('-' * 50)
    print('-'*50)

"""# **Linear Regression Model**"""

lr_pipeline = Pipeline([
    ('preprocessor', ct),
    ('model', LinearRegression())
])

lr_pipeline.fit(X_train, y_train)
y_pred_lr = lr_pipeline.predict(X_test)


from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

mse_lr = mean_squared_error(y_test, y_pred_lr)
rmse_lr = np.sqrt(mse_lr)
mae_lr = mean_absolute_error(y_test, y_pred_lr)
r2_lr = r2_score(y_test, y_pred_lr)

print(f"Linear Regression Model")
print_model_metrics(y_test, y_pred_lr, "Linear Regression")
plot_model_performance(y_test, y_pred_lr, "Linear Regression")

"""# **Random Forest Regressor Model**"""

from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(random_state=42)
rf_pipeline = Pipeline([
    ('preprocessor', ct),
    ('model', rf_model)
])

rf_pipeline.fit(X_train, y_train)
y_pred_rf = rf_pipeline.predict(X_test)


mse_rf = mean_squared_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mse_rf)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)


print_model_metrics(y_test, y_pred_rf, "Random Forest Regressor")
plot_model_performance(y_test, y_pred_rf, "Random Forest Regressor")

"""# **K-Nearest Neighbors Model**"""

from sklearn.neighbors import KNeighborsRegressor


knn_model = KNeighborsRegressor(n_neighbors=5)
knn_pipeline = Pipeline([
    ('preprocessor', ct),
    ('model', knn_model)
])

knn_pipeline.fit(X_train, y_train)
y_pred_knn = knn_pipeline.predict(X_test)

mse_knn = mean_squared_error(y_test, y_pred_knn)
rmse_knn = np.sqrt(mse_knn)
mae_knn = mean_absolute_error(y_test, y_pred_knn)
r2_knn = r2_score(y_test, y_pred_knn)

print_model_metrics(y_test, y_pred_knn, "KNN")
plot_model_performance(y_test, y_pred_knn, "KNN")

"""# **Decision Tree Regressor Model**"""

from sklearn.tree import DecisionTreeRegressor


dt_model = DecisionTreeRegressor(random_state=42)
dt_pipeline = Pipeline([
    ('preprocessor', ct),
    ('model', dt_model)
])

dt_pipeline.fit(X_train, y_train)
y_pred_dt = dt_pipeline.predict(X_test)


mse_dt = mean_squared_error(y_test, y_pred_dt)
rmse_dt = np.sqrt(mse_dt)
mae_dt = mean_absolute_error(y_test, y_pred_dt)
r2_dt = r2_score(y_test, y_pred_dt)




print_model_metrics(y_test, y_pred_dt, "Decision Tree Regressor")
plot_model_performance(y_test, y_pred_dt, "Decision Tree Regressor")

"""# **Comparison**"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

results = {}



# Store the metrics for each model in the 'results' dictionary
results["Linear Regression"] = {"R²": r2_score(y_test, y_pred_lr), "MAE": mean_absolute_error(y_test, y_pred_lr), "MSE": mean_squared_error(y_test, y_pred_lr), "RMSE": np.sqrt(mean_squared_error(y_test, y_pred_lr))}
results["Random Forest"] = {"R²": r2_score(y_test, y_pred_rf), "MAE": mean_absolute_error(y_test, y_pred_rf), "MSE": mean_squared_error(y_test, y_pred_rf), "RMSE": np.sqrt(mean_squared_error(y_test, y_pred_rf))}
results["K-Nearest Neighbors"] = {"R²": r2_score(y_test, y_pred_knn), "MAE": mean_absolute_error(y_test, y_pred_knn), "MSE": mean_squared_error(y_test, y_pred_knn), "RMSE": np.sqrt(mean_squared_error(y_test, y_pred_knn))}
results["Decision Tree"] = {"R²": r2_score(y_test, y_pred_dt), "MAE": mean_absolute_error(y_test, y_pred_dt), "MSE": mean_squared_error(y_test, y_pred_dt), "RMSE": np.sqrt(mean_squared_error(y_test, y_pred_dt))}

# Step 1: Metrics for each model
metrics_data = {
    "Model": ["Linear Regression", "Random Forest", "KNN", "Decision Tree"],
    "R^2": [results["Linear Regression"]["R²"],
            results["Random Forest"]["R²"],
            results["K-Nearest Neighbors"]["R²"],
            results["Decision Tree"]["R²"]],
    "MAE": [results["Linear Regression"]["MAE"],
            results["Random Forest"]["MAE"],
            results["K-Nearest Neighbors"]["MAE"],
            results["Decision Tree"]["MAE"]],
    "MSE": [results["Linear Regression"]["MSE"],
            results["Random Forest"]["MSE"],
            results["K-Nearest Neighbors"]["MSE"],
            results["Decision Tree"]["MSE"]],
    "RMSE": [results["Linear Regression"]["RMSE"],
             results["Random Forest"]["RMSE"],
             results["K-Nearest Neighbors"]["RMSE"],
             results["Decision Tree"]["RMSE"]]
}

metrics_df = pd.DataFrame(metrics_data)



print("Model Performance Comparison")
print(metrics_df)


plt.figure(figsize=(12, 12))

# Bar plot for R^2
plt.subplot(2, 2, 1)
sns.barplot(x="Model", y="R^2", data=metrics_df, palette="viridis")
plt.title("Model Comparison: R^2 Scores")
plt.ylabel("R^2 Score")
plt.xlabel("Model")
for index, value in enumerate(metrics_df["R^2"]):
    plt.text(index, value + 0.01, f"{value:.2f}", ha='center', va='bottom', fontsize=10)

# Bar plot for RMSE
plt.subplot(2, 2, 2)
sns.barplot(x="Model", y="RMSE", data=metrics_df, palette="viridis")
plt.title("Model Comparison: RMSE")
plt.ylabel("RMSE")
plt.xlabel("Model")
for index, value in enumerate(metrics_df["RMSE"]):
    plt.text(index, value + 0.01, f"{value:.2f}", ha='center', va='bottom', fontsize=10)

# Bar plot for MAE
plt.subplot(2, 2, 3)
sns.barplot(x="Model", y="MAE", data=metrics_df, palette="coolwarm")
plt.title("Model Comparison: MAE")
plt.ylabel("MAE")
plt.xlabel("Model")
for index, value in enumerate(metrics_df["MAE"]):
    plt.text(index, value + 0.01, f"{value:.2f}", ha='center', va='bottom', fontsize=10)

# Bar plot for MSE
plt.subplot(2, 2, 4)
sns.barplot(x="Model", y="MSE", data=metrics_df, palette="coolwarm")
plt.title("Model Comparison: MSE")
plt.ylabel("MSE")
plt.xlabel("Model")
for index, value in enumerate(metrics_df["MSE"]):
    plt.text(index, value + 0.01, f"{value:.2f}", ha='center', va='bottom', fontsize=10)

plt.tight_layout()
plt.show()

from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.preprocessing import KBinsDiscretizer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline


bins = 3
binner = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy='uniform')
y_binned = binner.fit_transform(y.values.reshape(-1, 1)).astype(int).flatten()


X_train, X_test, y_train_binned, y_test_binned = train_test_split(
    X, y_binned, test_size=0.3, random_state=42
)


models = {
    "Linear Regression": Pipeline([
        ('preprocessor', ct),
        ('model', LinearRegression())
    ]),
    "Random Forest": Pipeline([
        ('preprocessor', ct),
        ('model', RandomForestRegressor(random_state=42))
    ]),
    "KNN": Pipeline([
        ('preprocessor', ct),
        ('model', KNeighborsRegressor())
    ]),
    "Decision Tree": Pipeline([
        ('preprocessor', ct),
        ('model', DecisionTreeRegressor(random_state=42))
    ]),
}


f1_results = {}

for name, pipeline in models.items():

    pipeline.fit(X_train, y_train_binned)
    y_pred_binned = pipeline.predict(X_test)


    y_pred_binned = np.round(y_pred_binned).astype(int)
    y_pred_binned = np.clip(y_pred_binned, 0, bins - 1)


    precision = precision_score(y_test_binned, y_pred_binned, average='macro')
    recall = recall_score(y_test_binned, y_pred_binned, average='macro')
    f1 = f1_score(y_test_binned, y_pred_binned, average='macro')

    f1_results[name] = {
        "Precision": precision,
        "Recall": recall,
        "F1 Score": f1,
    }


for name, metrics in f1_results.items():
    print(f"{name}:\n  Precision: {metrics['Precision']:.2f}\n  Recall: {metrics['Recall']:.2f}\n  F1 Score: {metrics['F1 Score']:.2f}\n")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Function to display confusion matrix
def plot_confusion_matrix(y_test, y_pred, model_name):
    """
    Plots and prints the confusion matrix for a given model.
    """
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap='Blues')
    plt.title(f"Confusion Matrix for {model_name}")
    plt.show()


for name, pipeline in models.items():

    pipeline.fit(X_train, y_train_binned)
    y_pred_binned = pipeline.predict(X_test)


    y_pred_binned = np.round(y_pred_binned).astype(int)
    y_pred_binned = np.clip(y_pred_binned, 0, bins - 1)


    print(f"Confusion Matrix for {name}:")
    plot_confusion_matrix(y_test_binned, y_pred_binned, name)

from sklearn.metrics import confusion_matrix

# Function to display confusion matrix in tabular form
def print_confusion_matrix(y_test, y_pred, model_name):
    """
    Prints the confusion matrix for a given model in a tabular format.
    """
    cm = confusion_matrix(y_test, y_pred)
    print(f"Confusion Matrix for {model_name}:")
    print("Actual \\ Predicted |", " | ".join([f"Class {i}" for i in range(cm.shape[1])]))
    print("-" * (19 + cm.shape[1] * 8))  # Adjust for formatting
    for i, row in enumerate(cm):
        print(f"Class {i:<2}             |", " | ".join([f"{x:<6}" for x in row]))
    print("\n")

# Generate tabular confusion matrix for each model
for name, pipeline in models.items():
    # Train the model
    pipeline.fit(X_train, y_train_binned)
    y_pred_binned = pipeline.predict(X_test)

    # Convert predictions to nearest bins
    y_pred_binned = np.round(y_pred_binned).astype(int)
    y_pred_binned = np.clip(y_pred_binned, 0, bins - 1)  # Ensure predictions are within valid bin range

    # Print confusion matrix
    print_confusion_matrix(y_test_binned, y_pred_binned, name)